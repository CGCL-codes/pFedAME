server_config:
  setting: cross-device
  # learning strategy for serve: str; currently supported: {FeaAvg, FedDiscrete}
  strategy: None
  # number of communication roundsï¼š int
  num_rounds: 500
  # number of total participating clients: int
  num_clients: 100
  # control numper of participating clients per round: float [0, 1.0];
  participate_ratio: 0.1
  # randomly remove `drop_ratio` fraction of total participating clients at the aggregation time of each round: float [0.0, 1.0)
  drop_ratio: 0.0
  test_every: 1
  use_tqdm: False
  # dataset used for training: str; currently supported: {Mnist, FashionMnist, Cifar10}
  dataset: Cifar100
  # dataset partition strategy: str; {'iid-equal-size', 'iid-diff-size', 'noniid-label-quantity', 'noniid-label-distribution', 'shards'}
  partition: None
  # float >0
  beta: None
  # int <= 10
  num_classes_per_client: None
  # int
  num_shards_per_client: None
  num_classes: 100
  learning_rate: 1.0
  lr_decay_per_round: 1.0
  # layers to be skipped in aggregation
  exclude: !!python/tuple []

  # algorithm specific settings go here
  alpha: 0.01
  # FedPHP
  s_mu: 0.5
  fix_mu: True
  # Ensemble
  ensemble_local: True
  ensemble_global: True
  testglo: False
  # pFedAME
  adaptive_lr: 1.0
  adaptive_round: 1
  distillation_round: 1
  distillation_lr: 0.01
  distillation_model: loc # glo, per, avg, weight_avg
  hard_label_alpha: 1.0
  soft_label_beta: 0.1



client_config:
  # network used for each client
  model: Conv2Cifar
  # network specific setting goes here

  # dataset setting
  # input size: size of a single input; 3 channel, 32*32
  input_size: !!python/tuple [3, 32, 32]
  num_classes: 100
  # number of local epochs: int
  num_epochs: 5
  # number of samples per batch: int
  batch_size: 64
  # {Adam, SGD}
  optimizer: SGD
  # initial learning rate for each round
  learning_rate: 0.05
  lr_scheduler: diminishing
  lr_decay_per_round: 1.0
  num_rounds: 500
  participate_ratio: 0.1
  # other settings
  use_tqdm: False
  return_embedding: False

  # algorithm specific settings go here

  # pFedMe:
  lambda_pfedme: 1
  local_steps_pfedme: 5
  # pmodel_lr and lgmodel_lr is the learning rate for the global model and personalized model on the client side
  # in the actual test I set both of them to be the same and derive them from the setup_optimizer function to make fare comparison
  pmodel_lr: 0.05
  lgmodel_lr: 0.05
  # FedRep:
  head_finetune_round: 1
  # FedProx
  mu: 0.001

  s_mu: 0.5
  fix_mu: True
  # Ensemble
  ensemble_local: True
  ensemble_global: True
  testglo: False
  # pFedAME
  adaptive_lr: 1.0
  adaptive_round: 1
  distillation_round: 1
  distillation_lr: 0.01
  distillation_model: loc # glo, per, avg, weight_avg
  hard_label_alpha: 1.0
  soft_label_beta: 0.1
